{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " using of tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'my',\n",
       " 'first',\n",
       " 'sentence',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'tokenize',\n",
       " 'the',\n",
       " 'words',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence='this is my first sentence in NLP. i want to tokenize the words.'\n",
    "nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Person Barack/NNP Hussein/NNP Obama/NNP II/NNP)\n",
      "  born/VBD\n",
      "  (Date August/NNP 4/CD ,/, 1961/CD)\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  American/JJ\n",
      "  politician/NN\n",
      "  who/WP\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  the/DT\n",
      "  44th/CD\n",
      "  (Person President/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Place United/NNP States/NNPS)\n",
      "  from/IN\n",
      "  (Date January/NNP 20/CD ,/, 2009/CD)\n",
      "  ,/,\n",
      "  to/TO\n",
      "  (Date January/NNP 20/CD ,/, 2017/CD)\n",
      "  ./.\n",
      "  A/DT\n",
      "  member/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Person Democratic/NNP Party/NNP)\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  first/JJ\n",
      "  African/JJ\n",
      "  (Person American/NNP)\n",
      "  to/TO\n",
      "  assume/VB\n",
      "  the/DT\n",
      "  presidency/NN\n",
      "  and/CC\n",
      "  previously/RB\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  a/DT\n",
      "  (Place United/NNP States/NNPS)\n",
      "  (Person Senator/NNP)\n",
      "  from/IN\n",
      "  (Person Illinois/NNP)\n",
      "  (/(\n",
      "  2005–2008/CD\n",
      "  )/)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "barack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician\n",
    "who served as the 44th President of \n",
    "the United States from January 20, 2009, to January 20, 2017.\n",
    "A member of the Democratic Party, he was the \n",
    "first African American to assume the presidency and previously\n",
    "served as a United States Senator from Illinois (2005–2008).\"\"\"\n",
    "grammar = r\"\"\"Place: {<NNP><NNPS>+}\n",
    "           Date: {<NNP><CD><,><CD>}\n",
    "           Person: {<NNP>+}\n",
    "           \"\"\"\n",
    "tokenised_barack = word_tokenize(barack)\n",
    "pos_list = pos_tag(tokenised_barack)\n",
    "regParser = RegexpParser(grammar)\n",
    "reg_lines = regParser.parse(pos_list)\n",
    "print(reg_lines)\n",
    "#reg_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n",
      "[('Barack', 'NN'), ('Hussein', 'NN'), ('Obama', 'NN'), ('II', 'NN'), ('born', 'NN'), ('August', 'NN'), ('4', 'NN'), (',', 'NN'), ('1961', 'NN'), (')', 'NN'), ('is', 'NN'), ('an', 'NN'), ('American', 'NN'), ('politician', 'NN'), ('who', 'NN'), ('served', 'NN'), ('as', 'NN'), ('the', 'NN'), ('44th', 'NN'), ('President', 'NN'), ('of', 'NN'), ('the', 'NN'), ('United', 'NN'), ('States', 'NN'), ('from', 'NN'), ('January', 'NN'), ('20', 'NN'), (',', 'NN'), ('2009', 'NN'), (',', 'NN'), ('to', 'NN'), ('January', 'NN'), ('20', 'NN'), (',', 'NN'), ('2017', 'NN'), ('.', 'NN'), ('A', 'NN'), ('member', 'NN'), ('of', 'NN'), ('the', 'NN'), ('Democratic', 'NN'), ('Party', 'NN'), (',', 'NN'), ('he', 'NN'), ('was', 'NN'), ('the', 'NN'), ('first', 'NN'), ('African', 'NN'), ('American', 'NN'), ('to', 'NN'), ('assume', 'NN'), ('the', 'NN'), ('presidency', 'NN'), ('and', 'NN'), ('previously', 'NN'), ('served', 'NN'), ('as', 'NN'), ('a', 'NN'), ('United', 'NN'), ('States', 'NN'), ('Senator', 'NN'), ('from', 'NN'), ('Illinois', 'NN'), ('(', 'NN'), ('2005–2008', 'NN'), (')', 'NN'), ('.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# importing a predefined corpus\n",
    "from nltk.corpus import brown\n",
    "# getting the most common tag in the brown corpus\n",
    "tags = [tag for (word,tag) in brown.tagged_words()]\n",
    "most_common_tag = nltk.FreqDist(tags).max()\n",
    "print(most_common_tag)\n",
    "#Prints NN which means the most common POS is noun\n",
    "# Using the most_common_tag as the input for DefaultTagger\n",
    "from nltk import DefaultTagger\n",
    "default_tagger = DefaultTagger(most_common_tag)\n",
    "def_tagged_barack = default_tagger.tag(tokenised_barack)\n",
    "print(def_tagged_barack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
      "[('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('was', None), ('jumped', None), ('over', None), ('by', None), ('the', None), ('quick', None), ('brown', None), ('fox', None)]\n",
      "[('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('was', 'VBD'), ('jumped', 'VBN'), ('over', 'IN'), ('by', 'IN'), ('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sent1=\"the quick brown fox jumps over the lazy dog\"\n",
    "training_tags=pos_tag(word_tokenize(sent1))\n",
    "\n",
    "print(training_tags)\n",
    "ngramTagger=nltk.NgramTagger(n=2,train=[training_tags])\n",
    "\n",
    "sent2=\"the lazy dog was jumped over by the quick brown fox\"\n",
    "sent2_tags=ngramTagger.tag(word_tokenize(sent2))\n",
    "\n",
    "print(sent2_tags)\n",
    "\n",
    "print(pos_tag(word_tokenize(sent2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Donald', None), ('John', 'NNP'), ('Trump', None), ('(', '('), ('born', 'VBN'), ('June', None), ('14', None), (',', ','), ('1946', 'CD'), (')', ')'), ('is', 'VBZ'), ('the', 'DT'), ('45th', None), ('and', 'CC'), ('current', None), ('President', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.'), ('Before', None), ('entering', None), ('politics', None), (',', ','), ('he', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('businessman', None), ('and', 'CC'), ('television', None), ('personality', None), ('.', '.'), ('Trump', None), ('was', 'VBD'), ('born', 'VBN'), ('and', 'CC'), ('raised', None), ('in', 'IN'), ('the', 'DT'), ('New', 'NNP'), ('York', None), ('City', None), ('borough', None), ('of', 'IN'), ('Queens', None), (',', ','), ('and', 'CC'), ('received', None), ('an', 'DT'), ('economics', None), ('degree', None), ('from', 'IN'), ('the', 'DT'), ('Wharton', None), ('School', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('University', 'NNP'), ('of', 'IN'), ('Pennsylvania', None), ('.', '.'), ('He', 'PRP'), ('took', None), ('charge', None), ('of', 'IN'), ('his', 'PRP$'), ('family', 'NN'), (\"'s\", 'POS'), ('real', None), ('estate', None), ('business', None), ('in', 'IN'), ('1971', None), (',', ','), ('renamed', None), ('it', None), ('The', None), ('Trump', None), ('Organization', None), (',', ','), ('and', 'CC'), ('expanded', None), ('it', None), ('from', 'IN'), ('Queens', None), ('and', 'CC'), ('Brooklyn', None), ('into', None), ('Manhattan', None), ('.', '.'), ('The', None), ('company', None), ('built', None), ('or', None), ('renovated', None), ('skyscrapers', None), (',', ','), ('hotels', None), (',', ','), ('casinos', None), (',', ','), ('and', 'CC'), ('golf', None), ('courses', None), ('.', '.'), ('Trump', None), ('later', 'RB'), ('started', None), ('various', None), ('side', None), ('ventures', None), (',', ','), ('including', None), ('licensing', None), ('his', 'PRP$'), ('name', None), ('for', 'IN'), ('real', None), ('estate', None), ('and', 'CC'), ('consumer', None), ('products', None), ('.', '.'), ('He', 'PRP'), ('managed', None), ('the', 'DT'), ('company', None), ('until', None), ('his', 'PRP$'), ('2017', 'CD'), ('inauguration', None), ('.', '.'), ('He', 'PRP'), ('co-authored', None), ('several', None), ('books', None), (',', ','), ('including', None), ('The', None), ('Art', None), ('of', 'IN'), ('the', 'DT'), ('Deal', None), ('.', '.'), ('He', 'PRP'), ('owned', None), ('the', 'DT'), ('Miss', None), ('Universe', None), ('and', 'CC'), ('Miss', None), ('USA', None), ('beauty', None), ('pageants', None), ('from', 'IN'), ('1996', None), ('to', 'TO'), ('2015', None), (',', ','), ('and', 'CC'), ('he', 'PRP'), ('produced', None), ('and', 'CC'), ('hosted', None), ('the', 'DT'), ('reality', None), ('television', None), ('show', None), ('The', None), ('Apprentice', None), ('from', 'IN'), ('2003', None), ('to', 'TO'), ('2015', None), ('.', '.'), ('Forbes', None), ('estimates', None), ('his', 'PRP$'), ('net', None), ('worth', None), ('to', 'TO'), ('be', 'VB'), ('$', None), ('3.1', None), ('billion', None), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "barack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician\n",
    "who served as the 44th President of \n",
    "the United States from January 20, 2009, to January 20, 2017.\n",
    "A member of the Democratic Party, he was the \n",
    "first African American to assume the presidency and previously\n",
    "served as a United States Senator from Illinois (2005–2008).\"\"\"\n",
    "bush = \"\"\"George Walker Bush (born July 6, 1946) is an American politician who served as the 43rd President\n",
    " of the United States from 2001 to 2009.\n",
    "He had previously served as the 46th Governor of Texas from 1995 to 2000.\n",
    "Bush was born New Haven, Connecticut, and grew up in Texas. \n",
    "After graduating from Yale University in 1968 and Harvard Business School in 1975, he worked in the oil industry.\n",
    "Bush married Laura Welch in 1977 and unsuccessfully ran for the U.S. House of Representatives shortly thereafter. \n",
    "He later co-owned the Texas Rangers baseball team before defeating Ann Richards in the 1994 Texas gubernatorial election. \n",
    "Bush was elected President of the United States in 2000 when he defeated Democratic incumbent \n",
    "Vice President Al Gore after a close and controversial win that involved a stopped recount in Florida. \n",
    "He became the fourth person to be elected president while receiving fewer popular votes than his opponent.\n",
    "Bush is a member of a prominent political family and is the eldest son of Barbara and George H. W. Bush, \n",
    "the 41st President of the United States. \n",
    "He is only the second president to assume the nation's highest office after his father, following the footsteps\n",
    " of John Adams and his son, John Quincy Adams.\n",
    "His brother, Jeb Bush, a former Governor of Florida, was a candidate for the Republican presidential nomination\n",
    " in the 2016 presidential election. \n",
    "His paternal grandfather, Prescott Bush, was a U.S. Senator from Connecticut.\"\"\"\n",
    "pos_tag_barack = pos_tag(word_tokenize(barack))\n",
    "pos_tag_bush = pos_tag(word_tokenize(bush))\n",
    "trump = \"\"\"Donald John Trump (born June 14, 1946) is the 45th and current President of the United States.\n",
    "Before entering politics, he was a businessman and television personality. \n",
    "Trump was born and raised in the New York City borough of Queens, and received an economics degree from the\n",
    " Wharton School of the University of Pennsylvania. \n",
    "He took charge of his family's real estate business in 1971, renamed it The Trump Organization, and expanded \n",
    "it from Queens and Brooklyn into Manhattan. \n",
    "The company built or renovated skyscrapers, hotels, casinos, and golf courses. \n",
    "Trump later started various side ventures, including licensing his name for real estate and consumer products.\n",
    "He managed the company until his 2017 inauguration. \n",
    "He co-authored several books, including The Art of the Deal. He owned the Miss Universe and Miss USA beauty \n",
    "pageants from 1996 to 2015, and he produced and hosted the reality television show The Apprentice from 2003 to 2015.\n",
    "Forbes estimates his net worth to be $3.1 billion.\"\"\"\n",
    "unigram_tag = nltk.UnigramTagger(train=[pos_tag_barack,pos_tag_bush])\n",
    "trump_tags = unigram_tag.tag(word_tokenize(trump))\n",
    "print(trump_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Donald', 'NN'), ('John', 'NNP'), ('Trump', 'NN'), ('(', '('), ('born', 'VBN'), ('June', 'NN'), ('14', 'CD'), (',', ','), ('1946', 'CD'), (')', ')'), ('is', 'VBZ'), ('the', 'DT'), ('45th', 'NN'), ('and', 'CC'), ('current', 'NN'), ('President', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.'), ('Before', 'NN'), ('entering', 'VBG'), ('politics', 'NNS'), (',', ','), ('he', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('businessman', 'NN'), ('and', 'CC'), ('television', 'NN'), ('personality', 'NN'), ('.', '.'), ('Trump', 'NN'), ('was', 'VBD'), ('born', 'VBN'), ('and', 'CC'), ('raised', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('New', 'NNP'), ('York', 'NN'), ('City', 'NN'), ('borough', 'NN'), ('of', 'IN'), ('Queens', 'NNS'), (',', ','), ('and', 'CC'), ('received', 'VBD'), ('an', 'DT'), ('economics', 'NNS'), ('degree', 'NN'), ('from', 'IN'), ('the', 'DT'), ('Wharton', 'NN'), ('School', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('University', 'NNP'), ('of', 'IN'), ('Pennsylvania', 'NN'), ('.', '.'), ('He', 'PRP'), ('took', 'NN'), ('charge', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('family', 'NN'), (\"'s\", 'POS'), ('real', 'NN'), ('estate', 'NN'), ('business', 'NNS'), ('in', 'IN'), ('1971', 'CD'), (',', ','), ('renamed', 'VBD'), ('it', 'NN'), ('The', 'NN'), ('Trump', 'NN'), ('Organization', 'NN'), (',', ','), ('and', 'CC'), ('expanded', 'VBD'), ('it', 'NN'), ('from', 'IN'), ('Queens', 'NNS'), ('and', 'CC'), ('Brooklyn', 'NN'), ('into', 'NN'), ('Manhattan', 'NN'), ('.', '.'), ('The', 'NN'), ('company', 'NN'), ('built', 'NN'), ('or', 'NN'), ('renovated', 'VBD'), ('skyscrapers', 'NNS'), (',', ','), ('hotels', 'NNS'), (',', ','), ('casinos', 'NNS'), (',', ','), ('and', 'CC'), ('golf', 'NN'), ('courses', 'VBZ'), ('.', '.'), ('Trump', 'NN'), ('later', 'RB'), ('started', 'VBD'), ('various', 'NNS'), ('side', 'NN'), ('ventures', 'VBZ'), (',', ','), ('including', 'VBG'), ('licensing', 'VBG'), ('his', 'PRP$'), ('name', 'NN'), ('for', 'IN'), ('real', 'NN'), ('estate', 'NN'), ('and', 'CC'), ('consumer', 'NN'), ('products', 'NNS'), ('.', '.'), ('He', 'PRP'), ('managed', 'VBD'), ('the', 'DT'), ('company', 'NN'), ('until', 'NN'), ('his', 'PRP$'), ('2017', 'CD'), ('inauguration', 'NN'), ('.', '.'), ('He', 'PRP'), ('co-authored', 'VBD'), ('several', 'NN'), ('books', 'NNS'), (',', ','), ('including', 'VBG'), ('The', 'NN'), ('Art', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Deal', 'NN'), ('.', '.'), ('He', 'PRP'), ('owned', 'VBD'), ('the', 'DT'), ('Miss', 'NNS'), ('Universe', 'NN'), ('and', 'CC'), ('Miss', 'NNS'), ('USA', 'NN'), ('beauty', 'NN'), ('pageants', 'NNS'), ('from', 'IN'), ('1996', 'CD'), ('to', 'TO'), ('2015', 'CD'), (',', ','), ('and', 'CC'), ('he', 'PRP'), ('produced', 'VBD'), ('and', 'CC'), ('hosted', 'VBD'), ('the', 'DT'), ('reality', 'NN'), ('television', 'NN'), ('show', 'NN'), ('The', 'NN'), ('Apprentice', 'NN'), ('from', 'IN'), ('2003', 'CD'), ('to', 'TO'), ('2015', 'CD'), ('.', '.'), ('Forbes', 'VBZ'), ('estimates', 'VBZ'), ('his', 'PRP$'), ('net', 'NN'), ('worth', 'NN'), ('to', 'TO'), ('be', 'VB'), ('$', 'NN'), ('3.1', 'CD'), ('billion', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "default_tagger  = DefaultTagger('NN')\n",
    "patterns = [\n",
    "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'[Aa][Nn][Dd]','CC'),           # conjugate and\n",
    "     (r'.*ed$', 'VBD'),                # simple past\n",
    "     (r',','comma'),                        # comma\n",
    "     (r'.*ould$', 'MD'),               # modals\n",
    "     (r'.*ing$', 'VBG'),               # gerunds\n",
    "     (r'.*s$', 'NNS'),                 # plural nouns\n",
    "    # You will need to add several such rules to make an efficient tagger\n",
    "]\n",
    "\n",
    "regexTagger=nltk.RegexpTagger(patterns,backoff=default_tagger)\n",
    "unigram_tag=nltk.UnigramTagger(train=[pos_tag_barack,pos_tag_bush],backoff=regexTagger)\n",
    "\n",
    "trump_tags=unigram_tag.tag(word_tokenize(trump))\n",
    "print(trump_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag each word present in sample3.txt file with its POS using a pipeline containing BigramTagger --> UnigramTagger --> RegexpTagger --> DefaultTagger. Note that --> indicates backoff.\n",
    "\n",
    "Use the data in the brown corpus as reference POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-5ea893e58ecc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mpos_brown\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mregexTagger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegexpTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbackoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_tagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0munigram_tagger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnigramTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos_brown\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbackoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregexTagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mbigram_tagger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNgramTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos_brown\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbackoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munigram_tagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train, model, backoff, cutoff, verbose)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mNgramTagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode_json_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n, train, model, backoff, cutoff, verbose)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode_json_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, tagged_corpus, cutoff, verbose)\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[1;31m# Record the event.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mtoken_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                 \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mcontext\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_context_to_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackoff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "txtFilePath='D:\\\\DataScience\\\\NLP\\\\sample3.txt'\n",
    "\n",
    "txtFile=open(txtFilePath,'r')\n",
    "text=''\n",
    "for line in txtFile:\n",
    "    text=text+line\n",
    "\n",
    "vocab=nltk.word_tokenize(text)\n",
    "default_tagger  = DefaultTagger('NN')\n",
    "patterns = [\n",
    "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'[Aa][Nn][Dd]','CC'),           # conjugate and\n",
    "     (r'.*ed$', 'VBD'),                # simple past\n",
    "     (r',','comma'),                        # comma\n",
    "     (r'.*ould$', 'MD'),               # modals\n",
    "     (r'.*ing$', 'VBG'),               # gerunds\n",
    "     (r'.*s$', 'NNS'),                 # plural nouns\n",
    "    # You will need to add several such rules to make an efficient tagger\n",
    "]\n",
    "pos_brown =  brown.tagged_words()\n",
    "regexTagger=nltk.RegexpTagger(patterns,backoff=default_tagger)\n",
    "unigram_tagger=nltk.UnigramTagger(train=[pos_brown],backoff=regexTagger)\n",
    "bigram_tagger=nltk.NgramTagger(n=2,train=[pos_brown],backoff=unigram_tagger)\n",
    "\n",
    "target_tags=bigram_tagger.tag(vocab)\n",
    "print(target_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\DataScience\\\\Udemy\\\\MLCourse'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barack.txt', 'bush.txt', 'trump.txt']\n",
      "[['Donald', 'John', 'Trump', '(', 'born', 'June', '14', ',', '1946', ')', 'is', 'the', '45th', 'and', 'current', 'President', 'of', 'the', 'United', 'States', '.'], ['Before', 'entering', 'politics', ',', 'he', 'was', 'a', 'businessman', 'and', 'television', 'personality', '.'], ...]\n",
      "['George', 'Walker', 'Bush', '(', 'born', 'July', '6', ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader \n",
    "path = \"../../text_docs/\"\n",
    "president_corpus = PlaintextCorpusReader(path,\".*\",encoding='utf-8')\n",
    "# Display the files in the corpus\n",
    "print(president_corpus.fileids()) #Prints ['barack.txt', 'bush.txt', 'trump.txt']\n",
    "#Display the sentences in a specific file\n",
    "print(president_corpus.sents('trump.txt'))\n",
    "#Display the sentences in all files of the corpus\n",
    "#print(president_corpus.sents())\n",
    "#Display the words in a specific file\n",
    "print(president_corpus.words('bush.txt'))\n",
    "#Display the words in the corpus\n",
    "# print(president_corpus.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  country  current  india  indians  is  minister  modi  narendra  of  \\\n",
      "0    1        1        0      1        1   1         0     0         0   0   \n",
      "1    0        0        1      1        0   1         1     1         1   1   \n",
      "\n",
      "   prime  proud  republic  shri  the  we  \n",
      "0      0      1         1     0    0   1  \n",
      "1      1      0         0     1    1   0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "sent1 = \"India is a republic country. We are proud Indians.\"\n",
    "sent2 = \"The current Prime Minister of India is Shri. Narendra Modi.\"\n",
    "count_vectorizer = CountVectorizer()\n",
    "dtm = count_vectorizer.fit_transform([sent1,sent2])\n",
    "print(pd.DataFrame(data=dtm.toarray(), columns=count_vectorizer.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7763932022500211\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "print(cosine(dtm[0].toarray(),dtm[1].toarray())) \n",
    "#Prints 0.7763932022500211 indicating that the two documents are not very similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        are   country   current     india   indians        is  minister  \\\n",
      "0  0.377628  0.377628  0.000000  0.268685  0.377628  0.268685  0.000000   \n",
      "1  0.000000  0.000000  0.333102  0.237005  0.000000  0.237005  0.333102   \n",
      "\n",
      "       modi  narendra        of     prime     proud  republic      shri  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.377628  0.377628  0.000000   \n",
      "1  0.333102  0.333102  0.333102  0.333102  0.000000  0.000000  0.333102   \n",
      "\n",
      "        the        we  \n",
      "0  0.000000  0.377628  \n",
      "1  0.333102  0.000000  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-6462005ce887>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#Exercise: Compute the cosine distance of these two documents using tfidf_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mcosine\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[1;31m# cosine distance is also referred to as 'uncentered correlation',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[1;31m#   or 'reflective correlation'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[1;34m(u, v, w, centered)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mumu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvmu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[0muv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m     \u001b[0muu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[0mvv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "sent1 = \"India is a republic country. We are proud Indians.\"\n",
    "sent2 = \"The current Prime Minister of India is Shri. Narendra Modi.\"\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform([sent1,sent2])\n",
    "print(pd.DataFrame(data=tfidf_vectors.toarray(),columns=tfidf_vectorizer.get_feature_names()))\n",
    "#Exercise: Compute the cosine distance of these two documents using tfidf_vectors\n",
    "\n",
    "print(cosine(tfidf_vectors[0],tfidf_vectors[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "['../../text_docs/barack.txt', '../../text_docs/bush.txt', '../../text_docs/multiple_files.zip', '../../text_docs/multiple_files/Nokia.txt', '../../text_docs/multiple_files/Samsung.txt', '../../text_docs/trump.txt']\n",
      "Distance between articles on Barack and Bush is: 0.6036204882219185\n",
      "Distance between articles on Barack and Trump is: 1.0\n",
      "Distance between articles on Bush and Trump is: 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print(sys.getfilesystemencoding())\n",
    "\n",
    "path = \"../../text_docs/\"\n",
    "president_corpus = PlaintextCorpusReader(path,\".*\",encoding=\"ISO-8859-1\")\n",
    "tf_idf = TfidfVectorizer(input='filename',encoding=\"ISO-8859-1\")\n",
    "files = [path+filename for filename in list(president_corpus.fileids())]\n",
    "print(files)\n",
    "tf_idf_matrix = tf_idf.fit_transform(raw_documents=files)\n",
    "barack = tf_idf_matrix.toarray()[0]\n",
    "bush = tf_idf_matrix.toarray()[1]\n",
    "trump = tf_idf_matrix.toarray()[2]\n",
    "from scipy.spatial.distance import cosine\n",
    "print(\"Distance between articles on Barack and Bush is:\", cosine(barack,bush))\n",
    "print(\"Distance between articles on Barack and Trump is:\", cosine(barack,trump))\n",
    "print(\"Distance between articles on Bush and Trump is:\", cosine(bush,trump))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the provided text files into a corpus and determine how similar they are to each other after converting them to \n",
    "\n",
    "Document Term Matrix using bag of words model \n",
    "\n",
    "Document Term Matrix using TF-IDF model.\n",
    "\n",
    "Experiment with the following metrics to compute the similarity - cosine distance, euclidean distance, jaccard distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------cosine distance-------------\n",
      "distance for bow model 0.3436874071314737\n",
      "distance for tfidf model 0.4854778350222103\n",
      "------------------euclidean distance-------------\n",
      "distance for bow model 34.249087579087416\n",
      "distance for tfidf model 0.9853708286956822\n",
      "------------------jaccard distance-------------\n",
      "distance for bow model 0.9290123456790124\n",
      "distance for tfidf model 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "path=\"../../text_docs/multiple_files/\"\n",
    "phone_corpus=PlaintextCorpusReader(path,\".*\",encoding=\"ISO-8859-1\")\n",
    "bow=CountVectorizer(input='filename',encoding=\"ISO-8859-1\")\n",
    "tf_idf=TfidfVectorizer(input='filename',encoding=\"ISO-8859-1\")\n",
    "files=[path+filename for filename in list(phone_corpus.fileids())]\n",
    "\n",
    "bow_matrix=bow.fit_transform(raw_documents=files)\n",
    "tf_idf_matrix=tf_idf.fit_transform(raw_documents=files)\n",
    "\n",
    "nokia_tfidf=tf_idf_matrix.toarray()[0]\n",
    "samsung_tfidf=tf_idf_matrix.toarray()[1]\n",
    "\n",
    "nokia_bow=bow_matrix.toarray()[0]\n",
    "samsung_bow=bow_matrix.toarray()[1]\n",
    "\n",
    "print('------------------cosine distance-------------')\n",
    "print('distance for bow model',cosine(nokia_bow,samsung_bow))\n",
    "print('distance for tfidf model',cosine(nokia_tfidf,samsung_tfidf))\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import jaccard\n",
    "\n",
    "print('------------------euclidean distance-------------')\n",
    "print('distance for bow model',euclidean(nokia_bow,samsung_bow))\n",
    "print('distance for tfidf model',euclidean(nokia_tfidf,samsung_tfidf))\n",
    "\n",
    "\n",
    "print('------------------jaccard distance-------------')\n",
    "print('distance for bow model',jaccard(nokia_bow,samsung_bow))\n",
    "print('distance for tfidf model',jaccard(nokia_tfidf,samsung_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knows', 'lol']\n",
      "['nw', 'sms', 'urgnt']\n",
      "['abt']\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"\"\"Just forced myself to eat a slice. I'm really not hungry tho. \n",
    "           Mark is getting worried. He knows I'm sick when I turn down pizza. Lol\"\"\"\n",
    "sent2 = \"I call you later, don't have nw. If urgnt, sms me.\"\n",
    "sent3 = \"Watching a telugu movie..wat abt u?\"\n",
    "\n",
    "def find_unusual_words(text):\n",
    "    text_vocab_set=set([w.lower() for w in text if w.isalpha()])\n",
    "    english_vocab_set=set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual_vocab=text_vocab_set-english_vocab_set\n",
    "    \n",
    "    return sorted(unusual_vocab)\n",
    "\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent1)))\n",
    "\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent2)))\n",
    "\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#spelling mistakes and possible suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knows': ['knoll',\n",
       "  'knower',\n",
       "  'knop',\n",
       "  'kiowa',\n",
       "  'anous',\n",
       "  'know',\n",
       "  'knowe',\n",
       "  'slows',\n",
       "  'snowy',\n",
       "  'enos',\n",
       "  'now',\n",
       "  'nous',\n",
       "  'knew',\n",
       "  'unown',\n",
       "  'kos',\n",
       "  'knosp',\n",
       "  'news',\n",
       "  'klops',\n",
       "  'known',\n",
       "  'snow',\n",
       "  'enow',\n",
       "  'nobs',\n",
       "  'knock',\n",
       "  'knout',\n",
       "  'nowy',\n",
       "  'snowl',\n",
       "  'nowt',\n",
       "  'snowk',\n",
       "  'knob',\n",
       "  'knot'],\n",
       " 'lol': ['lox',\n",
       "  'loo',\n",
       "  'lob',\n",
       "  'pol',\n",
       "  'lot',\n",
       "  'lop',\n",
       "  'tol',\n",
       "  'log',\n",
       "  'lo',\n",
       "  'lola',\n",
       "  'lof',\n",
       "  'loa',\n",
       "  'kol',\n",
       "  'lou',\n",
       "  'loll',\n",
       "  'sol',\n",
       "  'vol',\n",
       "  'col',\n",
       "  'loy',\n",
       "  'dol',\n",
       "  'lolo',\n",
       "  'low',\n",
       "  'gol',\n",
       "  'lod'],\n",
       " 'sms': ['sus', 'sis', 'sma'],\n",
       " 'urgnt': ['brunt',\n",
       "  'orant',\n",
       "  'rent',\n",
       "  'urgent',\n",
       "  'brant',\n",
       "  'crunt',\n",
       "  'burnt',\n",
       "  'unrent',\n",
       "  'front',\n",
       "  'urn',\n",
       "  'durant',\n",
       "  'ergot',\n",
       "  'argent',\n",
       "  'urent',\n",
       "  'runt',\n",
       "  'rant',\n",
       "  'turgent',\n",
       "  'drant',\n",
       "  'ungot',\n",
       "  'rynt',\n",
       "  'urine',\n",
       "  'upget',\n",
       "  'urena',\n",
       "  'surgent',\n",
       "  'trent',\n",
       "  'urna',\n",
       "  'jurant',\n",
       "  'urge',\n",
       "  'uran',\n",
       "  'urger',\n",
       "  'grunt',\n",
       "  'usent',\n",
       "  'grant',\n",
       "  'brent',\n",
       "  'urging',\n",
       "  'argot',\n",
       "  'prunt',\n",
       "  'print',\n",
       "  'arent',\n",
       "  'trant',\n",
       "  'unget'],\n",
       " 'abt': ['art',\n",
       "  'apt',\n",
       "  'alt',\n",
       "  'abu',\n",
       "  'abe',\n",
       "  'aft',\n",
       "  'abet',\n",
       "  'aby',\n",
       "  'aba',\n",
       "  'ant',\n",
       "  'abb',\n",
       "  'aht',\n",
       "  'abut',\n",
       "  'ast',\n",
       "  'amt',\n",
       "  'abo',\n",
       "  'at',\n",
       "  'ab',\n",
       "  'act',\n",
       "  'ait']}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unusual_words_found = ['knows', 'lol', 'nw', 'sms', 'urgnt', 'abt']\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "possible_suggestions={}\n",
    "english_vocab_set=set(w.lower() for w in nltk.corpus.words.words())\n",
    "\n",
    "for word in unusual_words_found:\n",
    "    for engword in english_vocab_set:\n",
    "        dist=edit_distance(word,engword)\n",
    "        if dist<len(word)/2:\n",
    "            if word not in possible_suggestions.keys():\n",
    "                possible_suggestions[word]=[engword]\n",
    "            else:\n",
    "                possible_suggestions[word].append(engword)\n",
    "                \n",
    "possible_suggestions\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##possible names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'John']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def names_in_text(text):\n",
    "    names=[]\n",
    "    word_set=set([i for i in text if i.isalpha()])\n",
    "    male_names=nltk.corpus.names.words('male.txt')\n",
    "    female_names=nltk.corpus.names.words('female.txt')\n",
    "    \n",
    "    for word in word_set:\n",
    "        if word in male_names or word in female_names:\n",
    "            names.append(word)\n",
    "            \n",
    "    return names\n",
    "sent1 = \"John and Mary go to the church every Sunday\"\n",
    "sent2 = \"No man has ever seen the dark side of the Moon\"\n",
    "print(names_in_text(word_tokenize(sent1)))\n",
    "print(names_in_text(word_tokenize(sent2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n",
      "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "[Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n",
      "0.2\n",
      "['goodness', 'goodness', 'commodity', 'trade_good', 'full', 'estimable', 'honorable', 'respectable', 'beneficial', 'just', 'upright', 'adept', 'expert', 'practiced', 'proficient', 'skillful', 'skilful', 'dear', 'near', 'dependable', 'safe', 'secure', 'right', 'ripe', 'well', 'effective', 'in_effect', 'in_force', 'serious', 'sound', 'salutary', 'honest', 'undecomposed', 'unspoiled', 'unspoilt', 'well', 'thoroughly', 'soundly']\n",
      "work\n",
      "deny\n",
      "abacus\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WordNetCorpusReader' object has no attribute 'hyponame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-0bab3ade5d9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyponame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dusk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'WordNetCorpusReader' object has no attribute 'hyponame'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "# Get all possible meanings of the word \"dog\n",
    "print(wn.synsets(\"dog\"))\n",
    "# Get all lemma names of \"dog\"\n",
    "# print(dog.lemma_names())\n",
    "#Get all hypernyms of \"dog\"\n",
    "print(wn.synset('dog.n.01').hypernyms())\n",
    "# A hypernym is the generic term where as a hyponym is a specific term\n",
    "# For the word dog, the hypernyms are 'canine' and 'domestic_animal'\n",
    "#Get all hyponyms of \"dog\"\n",
    "print(wn.synset('dog.n.01').hyponyms())\n",
    "# some of hyponyms are  \"pug\", \"puppy\", \"lap_dog\", etc..\n",
    "#Get the path similarity between to words - the method returns the shortest path in the taxonomy\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "print(cat.path_similarity(dog)) #Returns a value between 0 and 1. The higher the number, higher the similarity in path\n",
    "# wu and palmer similarity method. \n",
    "# \"\"\" Produces similarity values based on their Least Common Subsumer (most specific ancestor node) and \n",
    "#    the maximum depth in the taxonomy\"\"\"\n",
    "cat.wup_similarity(dog)\n",
    "# Get all synonyms of the word 'good'\n",
    "synonyms = []\n",
    "for syn in wn.synsets(\"good\"):\n",
    "    for word in syn.lemmas():\n",
    "        if word.name() != \"good\":\n",
    "            synonyms.append(word.name())\n",
    "print(synonyms)\n",
    "# Get all antonyms of the word \"good\"\n",
    "antonyms = []\n",
    "for syn in wn.synsets(\"good\"):\n",
    "    for word in syn.lemmas():\n",
    "        if word.name() != \"good\" and word.antonyms():\n",
    "            antonyms.append( word.antonyms()[0].name())\n",
    "# print(antonyms)\n",
    "# Return the base form (morphy) of a word \n",
    "print(wn.morphy(\"working\" , wn.VERB)) #Prints \"work\"\n",
    "print(wn.morphy(\"denied\" , wn.VERB)) #Prints \"deny\"\n",
    "print(wn.morphy(\"abaci\")) #Prints \"abacus\"\n",
    "\n",
    "\n",
    "print(wn.hyponame('dusk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the provided spam.csv file:\n",
    "\n",
    "Determine the total number of unusual words/spelling mistakes\n",
    "\n",
    "For every unusual word, find the 10 closest possible valid words.\n",
    "You can consider that the first and the last letter of the unusual word is not ab spelling mistake.\n",
    "If the word is 'tkae', then the closest words can be 'take','true','tree', etc. where the first and last character are the same (i.e. 't' and 'e')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3647\n"
     ]
    }
   ],
   "source": [
    "filepath='D:\\\\DataScience\\\\NLP\\\\'\n",
    "spamtxtFile=open(filepath+'spam.csv','r')\n",
    "spamtxt=''\n",
    "for line in spamtxtFile:\n",
    "    spamtxt=spamtxt+line\n",
    "vocab=nltk.wordpunct_tokenize(spamtxt)\n",
    "\n",
    "spam_vocab=set(w.lower() for w in vocab if w.isalpha() )\n",
    "english_vocab_set\n",
    "\n",
    "unusual_words=spam_vocab-english_vocab_set\n",
    "\n",
    "print(len(unusual_words))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
